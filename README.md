# FSTT-DA
Few-Shot Test-Time Domain Adaptation

# Prompt_TTA
Prompting for Test-time Adaptation


## Quickstart

```bash
# Install requirements
pip install -r requirements.txt

# Download & Copy CLIP's pretrained weights from
/home/ligu/projects/prompt_tta_sam_iclr2024/pretrained/openai_clip

# Run train script with the default config
python train.py
```

## Dataset download and preparation
**WILDS datasets**

Please follow the download instructions provided by [WILDS benchmark](https://github.com/p-lambda/wilds/) to download iWildCam, Camelyon17, FMoW and ProvertyMap.

**DomainNet**

1. Download the official [DomainNet benchmark](http://ai.bu.edu/M3SDA/).
2. Generate a metadata.csv for DomainNet by running the [WILDS preprocessing script](https://github.com/p-lambda/wilds/blob/472677590de351857197a9bf24958838c39c272b/dataset_preprocessing/domainnet/generate_metadata.py).

Please note, that we evaluate the **`official test split`** of DomainNet instead of the random portion of the target domain as in [DomainBed](https://github.com/facebookresearch/DomainBed) codebase.

<br>

## Project Structure

The directory structure of new project looks like this:

Reference: [lightning-hydra-template](https://github.com/ashleve/lightning-hydra-template/)

```
│
├── configs                   <- Hydra configs
│   ├── callbacks                <- Callbacks configs
│   ├── data                     <- Data configs
│   ├── debug                    <- Debugging configs
│   ├── experiment               <- Experiment configs
│   ├── extras                   <- Extra utilities configs
│   ├── hparams_search           <- Hyperparameter search configs
│   ├── hydra                    <- Hydra configs
│   ├── local                    <- Local configs
│   ├── logger                   <- Logger configs
│   ├── model                    <- Model configs
│   ├── paths                    <- Project paths configs
│   ├── trainer                  <- Trainer configs
│   │
│   ├── eval.yaml             <- Main config for evaluation
│   └── train.yaml            <- Main config for training
│
├── logs                   <- Logs generated by hydra and lightning loggers
│
├── notebooks              <- Jupyter notebooks. Naming convention is a number (for ordering),
│                             the creator's initials, and a short `-` delimited description,
│                             e.g. `1.0-jqp-initial-data-exploration.ipynb`.
│
├── scripts                <- Shell scripts
│
├── src                    <- Source code
│   ├── datasets                <- benchmark datasets
│   ├── models                  <- model components
│   ├── lightning               <- LightningModule, DataModule, Callbacks
│   ├── solver                  <- losses, solvers, schedulers
│   ├── utils                   <- Utility modules
├── train.py                    <- Run training
├── eval.py                     <- Run evaluation
│
├── .env.example              <- Example of file for storing private environment variables
├── .project-root             <- File for inferring the position of project root directory
├── requirements.txt          <- File for installing pip environment
└── README.md
```
<br>

## How to try with your custom experiment configs

**Basic workflow**
1. Write your experiment config, adding your custom configs that would override the default.
2. Run training with chosen experiment config:
   ```bash
   python train.py experiment=experiment.yaml
   ```
<be>


## <a name="cite"/> :clipboard: Citation

If you use this code in your research, please consider citing our paper:
```
@article{chi_2024_ICLR,
  title={Adapting to Distribution Shift by Visual Domain Prompt Generation},
  author={Zhixiang Chi, Li Gu, Tao Zhong, Huan Liu, Yuanhao Yu, Konstantinos N Plataniotis, Yang Wang},
  journal={Proceedings of the Twelfth International Conference on Learning Representations},
  year={2024}
}
```
